{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNnL+FzVjj1cMo7nG0FcYOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Longcodedao/NAS-With-RL/blob/main/NAS_Total.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvjRhy-elmgP",
        "outputId": "673e7d47-9c27-4e15-e3f3-dc88ef43c7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/AutoML/NAS/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2nWRznRl051",
        "outputId": "9f064e8d-bb30-46ac-d136-50d117855a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/AutoML/NAS/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Qmqsmjohl4II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Params:\n",
        "    NUM_EPOCHS = 50\n",
        "    ALPHA = 0.005\n",
        "    BATCH_SIZE = 64\n",
        "    HIDDEN_SIZE = 64    # Number of Hidden Units in Controller\n",
        "    BETA = 0.1          # The entropy bonus multiplier\n",
        "    INPUT_SIZE = 3\n",
        "    ACTION_SPACE = 2\n",
        "    NUM_STEPS = 4\n",
        "    GAMMA = 0.99\n"
      ],
      "metadata": {
        "id": "z4rSHU_ymxY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5, ))]\n",
        ")\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root = './data', train = True,\n",
        "                                      download = True, transform = transform)\n",
        "testset = torchvision.datasets.MNIST(root = './data', train = False,\n",
        "                                     download = True, transform = transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64,\n",
        "                                          shuffle = True, num_workers = 2)\n",
        "testlaoder = torch.utils.data.DataLoader(testset, batch_size = 64,\n",
        "                                         shuffle = False, num_workers = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpzTGSSAnldo",
        "outputId": "358f2b9f-61be-4d15-c8c2-2b9bb4ccc605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 143049319.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 37433774.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 58481877.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7040106.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Controller(nn.Module):\n",
        "    def __init__(self, search_space,\n",
        "                hidden_size = 64, max_layer = 4, device = ''):\n",
        "\n",
        "        super(Controller, self).__init__()\n",
        "\n",
        "        self.search_space = search_space\n",
        "        self.DEVICE = device\n",
        "        self.hidden_size = hidden_size\n",
        "        self.length_search = len(search_space)     # num_steps = max_layer * length_search_space\n",
        "        self.list_length = [len(space) for space in search_space.values()]\n",
        "        self.max_layer = max_layer\n",
        "\n",
        "        self.lstm = nn.ModuleList()\n",
        "        self.fc = nn.ModuleList()\n",
        "\n",
        "        self.lstm.append(nn.LSTMCell(self.list_length[-1], self.hidden_size).to(self.DEVICE))\n",
        "\n",
        "        for i in range(1, self.length_search):\n",
        "            self.lstm.append(nn.LSTMCell(self.list_length[i - 1], self.hidden_size).to(self.DEVICE))\n",
        "\n",
        "\n",
        "        for i in range(0, self.length_search):\n",
        "            self.fc.append(nn.Linear(self.hidden_size, self.list_length[i]).to(self.DEVICE))\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h_t = torch.zeros(1, self.hidden_size, dtype = torch.float, device = self.DEVICE)\n",
        "        c_t = torch.zeros(1, self.hidden_size, dtype = torch.float, device = self.DEVICE)\n",
        "\n",
        "        return (h_t, c_t)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # self.total_layer = torch.randint(1, self.max_layer, (1,)).item()\n",
        "        outputs = {}\n",
        "\n",
        "        self.hidden = [self.init_hidden() for _ in range(self.length_search)]\n",
        "\n",
        "\n",
        "        for num_layer in range(self.max_layer):\n",
        "\n",
        "            for i, (key, val) in enumerate(self.search_space.items()):\n",
        "                h_t, c_t = self.hidden[i]\n",
        "                h_t, c_t = self.lstm[i](input, (h_t, c_t))\n",
        "                self.hidden[i] = (h_t, c_t)\n",
        "                output = self.fc[i](h_t)\n",
        "                # print(output)\n",
        "                input = output\n",
        "\n",
        "                if key not in outputs.keys():\n",
        "                  outputs[key] = [output]\n",
        "                else:\n",
        "                  outputs[key].extend([output])\n",
        "\n",
        "        # print(outputs)`\n",
        "\n",
        "        # for _ in range(self.length_search):\n",
        "        #     h_t, c_t = self.hidden[i]\n",
        "        #     h_t.detach_()\n",
        "        #     c_t.detach_()\n",
        "        #     self.hidden[i] = (h_t, c_t)\n",
        "\n",
        "        for i, (key, val) in enumerate(outputs.items()):\n",
        "            outputs[key] = torch.stack(outputs[key]).squeeze(1)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "fO2vNixaoY2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0: nn.ReLU, 1: nn.Tanh, 2: nn.Sigmoid\n",
        "\n",
        "search_space = {\n",
        "    \"hidden_units\": [8, 16, 32, 64],\n",
        "    \"activation\": [0, 1, 2]\n",
        "}\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "controller = Controller(search_space, max_layer = 4, device = device)\n",
        "print(f\"Total Layer: {controller.total_layer}\")\n",
        "print(f\"List Length: {controller.list_length}\")\n",
        "print(controller)\n",
        "input = torch.tensor([[1.0, 2.0, 3.0]]).to(device)\n",
        "outputs = controller(input)\n",
        "# print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvkzFpkNxbfx",
        "outputId": "613871a4-659e-46fd-c1d6-1078781841ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Layer: 2\n",
            "List Length: [4, 3]\n",
            "Controller(\n",
            "  (lstm): ModuleList(\n",
            "    (0): LSTMCell(3, 64)\n",
            "    (1): LSTMCell(4, 64)\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NASModel(nn.Module):\n",
        "    def __init__(self, architectures, input_size, output_size):\n",
        "        super(NASModel, self).__init__()\n",
        "        self.architectures = architectures\n",
        "        self.length_layers = len(self.architectures['hidden_units'])\n",
        "        self.output_size = output_size\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for layer in range(self.length_layers):\n",
        "          hidden_units = self.architectures['hidden_units'][layer].item()\n",
        "          activation = self.architectures['activation'][layer].item()\n",
        "          # print(activation)\n",
        "\n",
        "          if (activation == 0):\n",
        "            activation = nn.ReLU()\n",
        "          elif (activation == 1):\n",
        "            activation = nn.Tanh()\n",
        "          elif (activation == 2):\n",
        "            activation = nn.Sigmoid()\n",
        "\n",
        "          if layer == 0:\n",
        "            layers.append(nn.Linear(input_size, hidden_units))\n",
        "            layers.append(activation)\n",
        "\n",
        "          else:\n",
        "            layers.append(nn.Linear(self.architectures['hidden_units'][layer - 1].item(),\n",
        "                                    hidden_units))\n",
        "            layers.append(activation)\n",
        "\n",
        "        layers.append(nn.Linear(self.architectures['hidden_units'][self.length_layers - 1].item(), self.output_size))\n",
        "        layers.append(nn.Softmax(dim = 1))\n",
        "\n",
        "        # print(layers)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.model(x)"
      ],
      "metadata": {
        "id": "lQvW1kyn5v5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define an architecture"
      ],
      "metadata": {
        "id": "tD6HQ8257saZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
        "\n",
        "architecture = {}\n",
        "episode_total_log_probs = {}\n",
        "controller = Controller(search_space, max_layer = 4, device = device)\n",
        "episode_logits = controller(input)\n",
        "\n",
        "print(f\"Number of layers is: {controller.total_layer}\")\n",
        "for key, space in search_space.items():\n",
        "    logits = episode_logits[key]\n",
        "\n",
        "    action_index = Categorical(logits = logits).sample().unsqueeze(1)\n",
        "    # print(action_index)\n",
        "    actions_space = torch.tensor([space] * controller.total_layer).to(device)\n",
        "    action = torch.gather(actions_space, 1, action_index).to(device)\n",
        "    architecture[key] = action.squeeze(1)\n",
        "\n",
        "    # print(action_index.int().squeeze(1))\n",
        "\n",
        "    mask = one_hot(action_index, num_classes = len(space))\n",
        "    episode_log_probs = torch.sum(mask.float() * log_softmax(logits, dim = 1), dim = 1)\n",
        "    episode_total_log_probs[key] = episode_log_probs\n",
        "\n",
        "\n",
        "print(architecture)\n",
        "\n",
        "print(episode_total_log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvOVeDjL7rcL",
        "outputId": "e549907e-0a92-4330-b27d-04532fa83c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers is: 2\n",
            "{'hidden_units': tensor([16, 16], device='cuda:0'), 'activation': tensor([1, 2], device='cuda:0')}\n",
            "{'hidden_units': tensor([[ 0.0000, -2.5870,  0.0000,  0.0000],\n",
            "        [ 0.0000, -2.5870,  0.0000,  0.0000]], device='cuda:0',\n",
            "       grad_fn=<SumBackward1>), 'activation': tensor([[ 0.0000, -2.3228,  0.0000],\n",
            "        [ 0.0000,  0.0000, -2.1969]], device='cuda:0', grad_fn=<SumBackward1>)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NASModel(architecture, 784, 10)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydvS80wNFaMz",
        "outputId": "dfd7ff57-736a-45df-edc4-b43f3b66d69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=16, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=16, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "def play_episode(controller):\n",
        "    architecture = {}\n",
        "    episode_total_log_probs = {}\n",
        "\n",
        "    input = torch.tensor([[1.0, 2.0, 3.0]]).to(device)\n",
        "\n",
        "    # print(controller)\n",
        "    episode_logits = controller(input)\n",
        "\n",
        "\n",
        "    for key, space in search_space.items():\n",
        "        logits = episode_logits[key]\n",
        "\n",
        "        action_index = Categorical(logits = logits).sample().unsqueeze(1)\n",
        "        actions_space = torch.tensor([space] * controller.total_layer).to(device)\n",
        "        action = torch.gather(actions_space, 1, action_index).to(device)\n",
        "        architecture[key] = action.squeeze(1)\n",
        "\n",
        "        # print(action_index.int().squeeze(1))\n",
        "\n",
        "        mask = one_hot(action_index, num_classes = len(space))\n",
        "        episode_log_probs = torch.sum(mask.float() * log_softmax(logits, dim = 1), dim = 1)\n",
        "        episode_total_log_probs[key] = episode_log_probs\n",
        "\n",
        "    model = NASModel(architecture, 784, 10).to(device)\n",
        "    print(f'{model}\\n')\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr = 0.005, momentum = 0.9)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader):\n",
        "\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            inputs = inputs.view(-1, 784)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        running_loss /= len(trainloader)\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {running_loss}\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testlaoder:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images.view(-1, 784))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print('Accuracy of the network on the 10000 test images: {}'.format(acc))\n",
        "\n",
        "    # compute the reward\n",
        "    reward = acc\n",
        "\n",
        "    reward = torch.tensor(reward, device=device).detach()\n",
        "\n",
        "    sum_weighted_log_probs = {}\n",
        "\n",
        "\n",
        "\n",
        "    sum_weighted_log_probs['hidden_units'] = torch.sum(-episode_total_log_probs['hidden_units'] * reward).unsqueeze(0)\n",
        "    sum_weighted_log_probs['activation'] = torch.sum(-episode_total_log_probs['activation'] * reward).unsqueeze(0)\n",
        "\n",
        "    sum_weighted_loss = sum_weighted_log_probs['hidden_units'] + \\\n",
        "                        sum_weighted_log_probs['activation']\n",
        "\n",
        "    return sum_weighted_loss, episode_total_log_probs, reward"
      ],
      "metadata": {
        "id": "aveaZgCYKATZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "controller = Controller(search_space, max_layer = 4, device = device)\n",
        "print(controller)\n",
        "optimizer = optim.Adam(controller.parameters(), lr = 0.001)\n",
        "total_rewards = []\n",
        "\n",
        "controller.train()\n",
        "for epoch in range(10):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  epoch_log_probs = torch.empty((0,), device = device)\n",
        "\n",
        "  for i in range(3):\n",
        "    (sum_weighted_loss, episode_logits,\n",
        "        reward) = play_episode(controller)\n",
        "    print(sum_weighted_loss)\n",
        "    epoch_log_probs = torch.cat((epoch_log_probs, sum_weighted_loss))\n",
        "\n",
        "  loss = torch.mean(epoch_log_probs)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # for name, param in controller.named_parameters():\n",
        "  #   print(name, param.grad)\n",
        "\n",
        "  print(f\"Loss in {epoch} is: {loss}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nyu5sCNAQcVY",
        "outputId": "ed87bf93-0093-44a6-c690-e9800966ea20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Controller(\n",
            "  (lstm): ModuleList(\n",
            "    (0): LSTMCell(3, 64)\n",
            "    (1): LSTMCell(4, 64)\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (1): Linear(in_features=64, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
            "    (3): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.246454812316244\n",
            "Epoch 2: Loss = 2.0350197571427073\n",
            "Epoch 3: Loss = 1.8988345485252103\n",
            "Epoch 4: Loss = 1.8306730727651226\n",
            "Epoch 5: Loss = 1.8100813667911457\n",
            "Epoch 6: Loss = 1.8001896073060757\n",
            "Epoch 7: Loss = 1.7934741138903572\n",
            "Epoch 8: Loss = 1.7858665119102006\n",
            "Epoch 9: Loss = 1.7576661290390405\n",
            "Epoch 10: Loss = 1.7105653771459421\n",
            "Accuracy of the network on the 10000 test images: 87.82\n",
            "tensor([211.0522], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.3013808534089435\n",
            "Epoch 2: Loss = 2.2848092002400966\n",
            "Epoch 3: Loss = 2.1643705315935584\n",
            "Epoch 4: Loss = 2.1008799501827786\n",
            "Epoch 5: Loss = 1.9823337526463751\n",
            "Epoch 6: Loss = 1.7941777866278121\n",
            "Epoch 7: Loss = 1.74853507821748\n",
            "Epoch 8: Loss = 1.7207522429128699\n",
            "Epoch 9: Loss = 1.6671227295515634\n",
            "Epoch 10: Loss = 1.6520046978108665\n",
            "Accuracy of the network on the 10000 test images: 81.64\n",
            "tensor([1818.7146], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=8, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=8, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.2920622535860105\n",
            "Epoch 2: Loss = 2.0238296550982544\n",
            "Epoch 3: Loss = 1.7932758615977729\n",
            "Epoch 4: Loss = 1.7387574986099943\n",
            "Epoch 5: Loss = 1.725462046767603\n",
            "Epoch 6: Loss = 1.7171936984509548\n",
            "Epoch 7: Loss = 1.7148509823691362\n",
            "Epoch 8: Loss = 1.711187486201207\n",
            "Epoch 9: Loss = 1.7093981158758786\n",
            "Epoch 10: Loss = 1.7078650300182514\n",
            "Accuracy of the network on the 10000 test images: 74.4\n",
            "tensor([1630.3965], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Loss in 0 is: 1220.054443359375\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=16, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.2466551629719196\n",
            "Epoch 2: Loss = 1.8475843455745722\n",
            "Epoch 3: Loss = 1.6754565316476802\n",
            "Epoch 4: Loss = 1.6386525469548159\n",
            "Epoch 5: Loss = 1.6268924064219379\n",
            "Epoch 6: Loss = 1.6192489262582905\n",
            "Epoch 7: Loss = 1.614306011179617\n",
            "Epoch 8: Loss = 1.6088685958878572\n",
            "Epoch 9: Loss = 1.6054055467089101\n",
            "Epoch 10: Loss = 1.602544101697804\n",
            "Accuracy of the network on the 10000 test images: 86.01\n",
            "tensor([1979.7157], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.2906807503466413\n",
            "Epoch 2: Loss = 2.1927131071273704\n",
            "Epoch 3: Loss = 1.9692449033387434\n",
            "Epoch 4: Loss = 1.8228807026135134\n",
            "Epoch 5: Loss = 1.765804822511002\n",
            "Epoch 6: Loss = 1.728175131115578\n",
            "Epoch 7: Loss = 1.7153424512602882\n",
            "Epoch 8: Loss = 1.7083394650711434\n",
            "Epoch 9: Loss = 1.7033512120816245\n",
            "Epoch 10: Loss = 1.6998939997097577\n",
            "Accuracy of the network on the 10000 test images: 76.75\n",
            "tensor([732.8957], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=16, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.2836328249242004\n",
            "Epoch 2: Loss = 2.173613598860149\n",
            "Epoch 3: Loss = 2.078787506007945\n",
            "Epoch 4: Loss = 2.0208171969537796\n",
            "Epoch 5: Loss = 1.9463856302853078\n",
            "Epoch 6: Loss = 1.8943272368994348\n",
            "Epoch 7: Loss = 1.8771489571406643\n",
            "Epoch 8: Loss = 1.8500318640330706\n",
            "Epoch 9: Loss = 1.8187361874305872\n",
            "Epoch 10: Loss = 1.8015859085105375\n",
            "Accuracy of the network on the 10000 test images: 67.14\n",
            "tensor([690.6725], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Loss in 1 is: 1134.427978515625\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=8, out_features=32, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=32, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.3004897593943547\n",
            "Epoch 2: Loss = 2.2552316481116486\n",
            "Epoch 3: Loss = 2.1744056891784993\n",
            "Epoch 4: Loss = 1.9764075122916622\n",
            "Epoch 5: Loss = 1.8499251727356332\n",
            "Epoch 6: Loss = 1.8065264690151093\n",
            "Epoch 7: Loss = 1.7762597714151656\n",
            "Epoch 8: Loss = 1.7352403393432276\n",
            "Epoch 9: Loss = 1.7066903686218424\n",
            "Epoch 10: Loss = 1.667215501448747\n",
            "Accuracy of the network on the 10000 test images: 82.38\n",
            "tensor([1830.7700], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=32, out_features=8, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=8, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.288865581758495\n",
            "Epoch 2: Loss = 2.18873172401111\n",
            "Epoch 3: Loss = 2.040933333353193\n",
            "Epoch 4: Loss = 1.9248704312960985\n",
            "Epoch 5: Loss = 1.8379422108501768\n",
            "Epoch 6: Loss = 1.8110582819625514\n",
            "Epoch 7: Loss = 1.7837812545965475\n",
            "Epoch 8: Loss = 1.7290869203966055\n",
            "Epoch 9: Loss = 1.67471807051315\n",
            "Epoch 10: Loss = 1.649870584260172\n",
            "Accuracy of the network on the 10000 test images: 84.31\n",
            "tensor([826.7817], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=8, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.0581083197329346\n",
            "Epoch 2: Loss = 1.814635534149243\n",
            "Epoch 3: Loss = 1.7883501446831709\n",
            "Epoch 4: Loss = 1.7826898662266193\n",
            "Epoch 5: Loss = 1.7789241744002808\n",
            "Epoch 6: Loss = 1.7762240781458711\n",
            "Epoch 7: Loss = 1.7741926238735093\n",
            "Epoch 8: Loss = 1.7723542528111798\n",
            "Epoch 9: Loss = 1.7101631242074946\n",
            "Epoch 10: Loss = 1.694126815302794\n",
            "Accuracy of the network on the 10000 test images: 76.92\n",
            "tensor([752.5591], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Loss in 2 is: 1136.70361328125\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (5): Sigmoid()\n",
            "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.3011301339053905\n",
            "Epoch 2: Loss = 2.290179854517044\n",
            "Epoch 3: Loss = 2.225221014226169\n",
            "Epoch 4: Loss = 2.1642006671250757\n",
            "Epoch 5: Loss = 2.1404712689456655\n",
            "Epoch 6: Loss = 2.1304108176404224\n",
            "Epoch 7: Loss = 2.1251554548867477\n",
            "Epoch 8: Loss = 2.121275171288041\n",
            "Epoch 9: Loss = 2.1172130257844417\n",
            "Epoch 10: Loss = 2.1107533979517563\n",
            "Accuracy of the network on the 10000 test images: 38.79\n",
            "tensor([877.6656], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=16, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
            "    (5): Sigmoid()\n",
            "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (7): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.301964265197071\n",
            "Epoch 2: Loss = 2.3011616176125336\n",
            "Epoch 3: Loss = 2.3005840229327235\n",
            "Epoch 4: Loss = 2.2995061406702884\n",
            "Epoch 5: Loss = 2.294869160601325\n",
            "Epoch 6: Loss = 2.2580176167396595\n",
            "Epoch 7: Loss = 2.213764569906792\n",
            "Epoch 8: Loss = 2.205696801132739\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-37eebfdeaf4a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     (sum_weighted_loss, episode_logits, \n\u001b[0;32m---> 14\u001b[0;31m         reward) = play_episode(controller)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_weighted_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepoch_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_weighted_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-ce8ddd33e80f>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(controller)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([[1.0, 2.0, 3.0]]).to(device)\n",
        "controller(input)\n",
        "\n",
        "episode_logits = controller(input)\n",
        "architecture = {}\n",
        "episode_total_log_probs = {}\n",
        "\n",
        "for key, space in search_space.items():\n",
        "    logits = episode_logits[key]\n",
        "\n",
        "    action_index = Categorical(logits = logits).sample().unsqueeze(1)\n",
        "    actions_space = torch.tensor([space] * controller.total_layer).to(device)\n",
        "    action = torch.gather(actions_space, 1, action_index).to(device)\n",
        "    architecture[key] = action.squeeze(1)\n",
        "\n",
        "    # print(action_index.int().squeeze(1))\n",
        "\n",
        "    mask = one_hot(action_index, num_classes = len(space))\n",
        "    episode_log_probs = torch.sum(mask.float() * log_softmax(logits, dim = 1), dim = 1)\n",
        "    episode_total_log_probs[key] = episode_log_probs\n",
        "architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdZgVhHxakQi",
        "outputId": "85bfe3d5-d504-4a0c-cdfb-d12ccbb6c661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hidden_units': tensor([64,  8], device='cuda:0'),\n",
              " 'activation': tensor([1, 0], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NASModel(architecture, 784, 10).to(device)\n",
        "print(f'{model}\\n')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.005, momentum = 0.9)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        inputs = inputs.view(-1, 784)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    running_loss /= len(trainloader)\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {running_loss}\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testlaoder:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images.view(-1, 784))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "acc = 100 * correct / total\n",
        "print('Accuracy of the network on the 10000 test images: {}'.format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQXhXyAffznV",
        "outputId": "446482ff-065e-4c47-dbf9-47b81ad70e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NASModel(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=8, out_features=10, bias=True)\n",
            "    (5): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1: Loss = 2.1128573410038247\n",
            "Epoch 2: Loss = 1.7757354270674781\n",
            "Epoch 3: Loss = 1.7230752162587668\n",
            "Epoch 4: Loss = 1.7097431104828809\n",
            "Epoch 5: Loss = 1.7022325784158605\n",
            "Epoch 6: Loss = 1.6970807165225177\n",
            "Epoch 7: Loss = 1.6929669145073718\n",
            "Epoch 8: Loss = 1.6894590497525261\n",
            "Epoch 9: Loss = 1.6868822404316492\n",
            "Epoch 10: Loss = 1.6845199286556447\n",
            "Accuracy of the network on the 10000 test images: 77.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCB85DJN9OOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}